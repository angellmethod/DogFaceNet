{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6704e96d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:38:42.610229Z",
     "iopub.status.busy": "2024-10-07T01:38:42.609745Z",
     "iopub.status.idle": "2024-10-07T01:38:59.293765Z",
     "shell.execute_reply": "2024-10-07T01:38:59.291894Z"
    },
    "papermill": {
     "duration": 16.696536,
     "end_time": "2024-10-07T01:38:59.296624",
     "exception": false,
     "start_time": "2024-10-07T01:38:42.600088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/lightgbm-3-3-5/lightgbm-3.3.5-py3-none-manylinux1_x86_64.whl\r\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from lightgbm==3.3.5) (0.43.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from lightgbm==3.3.5) (1.26.4)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from lightgbm==3.3.5) (1.14.1)\r\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.10/site-packages (from lightgbm==3.3.5) (1.2.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn!=0.22.0->lightgbm==3.3.5) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn!=0.22.0->lightgbm==3.3.5) (3.5.0)\r\n",
      "Installing collected packages: lightgbm\r\n",
      "  Attempting uninstall: lightgbm\r\n",
      "    Found existing installation: lightgbm 4.2.0\r\n",
      "    Uninstalling lightgbm-4.2.0:\r\n",
      "      Successfully uninstalled lightgbm-4.2.0\r\n",
      "Successfully installed lightgbm-3.3.5\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/lightgbm-3-3-5/lightgbm-3.3.5-py3-none-manylinux1_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e986c44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:38:59.316404Z",
     "iopub.status.busy": "2024-10-07T01:38:59.315898Z",
     "iopub.status.idle": "2024-10-07T01:39:03.240857Z",
     "shell.execute_reply": "2024-10-07T01:39:03.239168Z"
    },
    "papermill": {
     "duration": 3.939884,
     "end_time": "2024-10-07T01:39:03.244615",
     "exception": false,
     "start_time": "2024-10-07T01:38:59.304731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##ordinalgbt code\n",
    "\"\"\"\n",
    "BSD 3-Clause License\n",
    "\n",
    "Copyright (c) 2023, adamingas\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without\n",
    "modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "1. Redistributions of source code must retain the above copyright notice, this\n",
    "   list of conditions and the following disclaimer.\n",
    "\n",
    "2. Redistributions in binary form must reproduce the above copyright notice,\n",
    "   this list of conditions and the following disclaimer in the documentation\n",
    "   and/or other materials provided with the distribution.\n",
    "\n",
    "3. Neither the name of the copyright holder nor the names of its\n",
    "   contributors may be used to endorse or promote products derived from\n",
    "   this software without specific prior written permission.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\"\"\"\n",
    "\n",
    "from functools import wraps\n",
    "from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def dec_clip_y_pred(fun):\n",
    "    @wraps(fun)\n",
    "    def wrapped(*, y_true, y_preds, theta):\n",
    "        y_preds = np.clip(y_preds, max(theta)-36, a_max=700 + min(theta))\n",
    "        return fun(y_true=y_true, y_preds=y_preds, theta=theta)\n",
    "\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def stack_zeros_ones(a: np.ndarray, only_zeros=False) -> np.ndarray:\n",
    "    \"\"\"Stacks zeroes and ones on the left and rights part of the array\n",
    "    Stacks horizontally zeros and ones on the left and right hand side of the array\n",
    "    respectively. If only_zeros is true then it only stacks zeros. This is for the\n",
    "    gradient which is zero at both ends of the sigmoid.\n",
    "    e.g.::\n",
    "\n",
    "        a = [[1,2],\n",
    "            [3,4],\n",
    "            [5,6]]\n",
    "        returns\n",
    "            [[0,1,2,1],\n",
    "            [0,3,4,1],\n",
    "            [0,5,6,1]]\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a: np.ndarray\n",
    "        A 2D array to pad with zeroes and ones\n",
    "    only_zeros: bool, default False\n",
    "        If true, then it pads with only zeroes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "\n",
    "    \"\"\"\n",
    "    if only_zeros:\n",
    "        return np.hstack(\n",
    "            (\n",
    "                np.zeros(a.shape[0])[:, np.newaxis],\n",
    "                a,\n",
    "                np.zeros(a.shape[0])[:, np.newaxis],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return np.hstack(\n",
    "        (np.zeros(a.shape[0])[:, np.newaxis], a, np.ones(a.shape[0])[:, np.newaxis])\n",
    "    )\n",
    "\n",
    "\n",
    "def sigmoid(z)->np.ndarray:\n",
    "    \"\"\"Sigmoid\n",
    "    Sigmoid implementation in numpy\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : np.ndarray\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def grad_sigmoid(z) -> np.ndarray:\n",
    "    \"\"\"Gradient\n",
    "    Gradient of the sigmoid\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : np.ndarray\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Gradient of Sigmoid\n",
    "\n",
    "    \"\"\"\n",
    "    phat = sigmoid(z)\n",
    "    return phat * (1 - phat)\n",
    "\n",
    "\n",
    "def hess_sigmoid(z) -> np.ndarray:\n",
    "    \"\"\"Hessian\n",
    "    Hessian of the sigmoid\n",
    "    Sigmoid implementation in numpy\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : np.ndarray\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Hessian of sigmoid\n",
    "\n",
    "    \"\"\"\n",
    "    grad = grad_sigmoid(z)\n",
    "    sig = sigmoid(z)\n",
    "    return grad * (1 - sig) - sig * (grad)\n",
    "\n",
    "\n",
    "def alpha2theta(alpha):  # theta[t] = theta[t-1] + exp(alpha[t])\n",
    "    return np.cumsum(np.append(alpha[0], np.exp(alpha[1:])))\n",
    "\n",
    "\n",
    "def theta2alpha(theta):  # alpha[t] = log(theta[t] - theta[t-1])\n",
    "    return np.append(theta[0], np.log(theta[1:] - theta[:-1]))\n",
    "\n",
    "\n",
    "def probas_from_y_pred(y_preds, theta):\n",
    "    \"\"\"\n",
    "    convers y_preds to probabilities\n",
    "    \"\"\"\n",
    "    s_array: np.ndarray = sigmoid(theta - y_preds[:, np.newaxis])\n",
    "    # Adding boundary terms of 1 and 0 to make sure that we have probabilities for\n",
    "    # all classes :TODO: Explain in detail\n",
    "    # Cumulative probabilities, for column k and row i this matrix represents\n",
    "    # P(y_i<=k)\n",
    "    c_probas = stack_zeros_ones(s_array)\n",
    "\n",
    "    probas = c_probas[:, 1 : len(theta) + 2] - c_probas[:, 0 : len(theta) + 1]\n",
    "    return probas\n",
    "\n",
    "@dec_clip_y_pred\n",
    "def ordinal_logistic_nll(y_true: np.ndarray, y_preds: np.ndarray, theta: np.ndarray):\n",
    "    \"\"\"Ordinal Negative log lilelihood\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        1-D array with correct labels, starts from 0 and goes up to the number\n",
    "        of unique classes minus one (so unique values are 0,1,2 when dealing\n",
    "        with three classes)\n",
    "    y_preds : np.ndarray\n",
    "        1-D array with predictions in latent space\n",
    "    theta : np.ndarray\n",
    "        thresholds, 1-D array, size is the number of classes minus one.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        logistic ordinal negative log likelihood\n",
    "\n",
    "    \"\"\"\n",
    "    probas = probas_from_y_pred(y_preds, theta)\n",
    "    # probabilities associated with the correct label\n",
    "    label_probas = probas[np.arange(0, len(y_true)), y_true]\n",
    "    label_probas = np.clip(\n",
    "        label_probas,\n",
    "        a_min=np.finfo(float).eps,\n",
    "        a_max=1 - len(theta) * np.finfo(float).eps\n",
    "    )\n",
    "    # loss\n",
    "    return -np.sum(np.log(label_probas))\n",
    "\n",
    "\n",
    "# Gradient\n",
    "def gradient_ordinal_logistic_nll(\n",
    "    y_true: np.ndarray, y_preds: np.ndarray, theta: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Gradient of ordinal nll\n",
    "    Gradient of the ordinal logistic regression with respect to the predictions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        1-D array with correct labels, starts from 0 and goes up to the number\n",
    "        of unique classes minus one (so unique values are 0,1,2 when dealing\n",
    "        with three classes)\n",
    "    y_preds : np.ndarray\n",
    "        1-D array with predictions in latent space\n",
    "    theta : np.ndarray\n",
    "        thresholds, 1-D array, size is the number of classes minus one.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Gradient of logistic ordinal negative log likelihood\n",
    "\n",
    "    \"\"\"\n",
    "    y_preds = np.clip(y_preds, -20, a_max=700 + min(theta))\n",
    "    probas = probas_from_y_pred(y_preds, theta)\n",
    "    y_true = y_true.astype(int)\n",
    "    gs_array: np.ndarray = grad_sigmoid(theta - y_preds[:, np.newaxis])\n",
    "    gc_probas = stack_zeros_ones(gs_array, only_zeros=True)\n",
    "    g_probas = -(gc_probas[:, 1 : len(theta) + 2] - gc_probas[:, 0 : len(theta) + 1])\n",
    "    gradient = -(g_probas / probas)[np.arange(0, len(y_true)), y_true]\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def hessian_ordinal_logistic_nll(\n",
    "    y_true: np.ndarray, y_preds: np.ndarray, theta: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Hessian of ordinal nll\n",
    "    Hessian of the ordinal logistic regression with respect to the predictions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        1-D array with correct labels, starts from 0 and goes up to the number\n",
    "        of unique classes minus one (so unique values are 0,1,2 when dealing\n",
    "        with three classes)\n",
    "    y_preds : np.ndarray\n",
    "        1-D array with predictions in latent space\n",
    "    theta : np.ndarray\n",
    "        thresholds, 1-D array, size is the number of classes minus one.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Hessian of logistic ordinal negative log likelihood\n",
    "\n",
    "    \"\"\"\n",
    "    y_preds = np.clip(y_preds, -20, a_max=700 + min(theta))\n",
    "    probas = probas_from_y_pred(y_preds, theta)\n",
    "\n",
    "    y_true = y_true.astype(int)\n",
    "    gs_array: np.ndarray = grad_sigmoid(theta - y_preds[:, np.newaxis])\n",
    "    gc_probas = stack_zeros_ones(gs_array, only_zeros=True)\n",
    "    hs_array: np.ndarray = hess_sigmoid(theta - y_preds[:, np.newaxis])\n",
    "    hc_probas = stack_zeros_ones(hs_array, only_zeros=True)\n",
    "    g_probas = -(gc_probas[:, 1 : len(theta) + 2] - gc_probas[:, 0 : len(theta) + 1])\n",
    "    h_probas = hc_probas[:, 1 : len(theta) + 2] - hc_probas[:, 0 : len(theta) + 1]\n",
    "\n",
    "    hessian = -(h_probas / probas - np.power(g_probas / probas, 2))[\n",
    "        np.arange(0, len(y_true)), y_true\n",
    "    ]\n",
    "    # hessian[np.abs(hessian) <=np.finfo(float).eps] = -np.finfo(float).eps\n",
    "    return hessian\n",
    "\n",
    "\n",
    "def lgb_ordinal_loss(\n",
    "    y_true: np.ndarray, y_pred: np.ndarray, theta: np.ndarray\n",
    "):\n",
    "    \"\"\"Ordinal loss for lightgbm use\n",
    "    The ordinal loss used in the lightgbm framework. Returns the\n",
    "    gradient and hessian of the loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        1-D array with correct labels, starts from 0 and goes up to the number\n",
    "        of unique classes minus one (so unique values are 0,1,2 when dealing\n",
    "        with three classes)\n",
    "    y_preds : np.ndarray\n",
    "        1-D array with predictions in latent space\n",
    "    theta : np.ndarray\n",
    "        thresholds, 1-D array, size is the number of classes minus one.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (np.ndarray, np.ndarray)\n",
    "        Gradient and Hessian of logistic ordinal negative log likelihood\n",
    "    \"\"\"\n",
    "    grad = gradient_ordinal_logistic_nll(y_true, y_pred, theta)\n",
    "    hess = hessian_ordinal_logistic_nll(y_true, y_pred, theta)\n",
    "    return (grad, hess)\n",
    "\n",
    "\n",
    "\n",
    "class LGBMOrdinal(LGBMRegressor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        #  threshold_interval: float=2,\n",
    "        boosting_type: str = \"gbdt\",\n",
    "        num_leaves: int = 31,\n",
    "        max_depth: int = -1,\n",
    "        learning_rate: float = 0.1,\n",
    "        n_estimators: int = 100,\n",
    "        subsample_for_bin: int = 200000,\n",
    "        objective=\"immediate-thresholds\",\n",
    "        class_weight=None,\n",
    "        min_split_gain: float = 0.0,\n",
    "        min_child_weight: float = 1e-3,\n",
    "        min_child_samples: int = 20,\n",
    "        subsample: float = 1.0,\n",
    "        subsample_freq: int = 0,\n",
    "        colsample_bytree: float = 1.0,\n",
    "        reg_alpha: float = 0.0,\n",
    "        reg_lambda: float = 0.0,\n",
    "        random_state=None,\n",
    "        n_jobs: int = -1,\n",
    "        silent=\"warn\",\n",
    "        importance_type: str = \"split\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            objective=None,\n",
    "            boosting_type=boosting_type,\n",
    "            num_leaves=num_leaves,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            n_estimators=n_estimators,\n",
    "            subsample_for_bin=subsample_for_bin,\n",
    "            class_weight=class_weight,\n",
    "            min_split_gain=min_split_gain,\n",
    "            min_child_weight=min_child_weight,\n",
    "            min_child_samples=min_child_samples,\n",
    "            subsample=subsample,\n",
    "            subsample_freq=subsample_freq,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            random_state=random_state,\n",
    "            n_jobs=n_jobs,\n",
    "            silent=silent,\n",
    "            importance_type=importance_type,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # self.threshold_interval = threshold_interval\n",
    "\n",
    "    def _initialise_theta(self):\n",
    "        return np.linspace(0, (self.n_classes - 2) * 1, self.n_classes - 1)\n",
    "\n",
    "    def _lgb_loss_factory(self):\n",
    "        self.theta = self._initialise_theta()\n",
    "        return self.loss  # Return the loss method\n",
    "\n",
    "    def loss(self, y_test, y_pred):\n",
    "        # Now `loss` is a class method, not a local function\n",
    "        return lgb_ordinal_loss(y_test, y_pred, self.theta)\n",
    "\n",
    "    @staticmethod\n",
    "    def _alpha_loss_factory(y_true, y_preds):\n",
    "        \"\"\"\n",
    "        Creates loss parametrised by alpha\n",
    "        \"\"\"\n",
    "\n",
    "        def loss(alpha):\n",
    "            theta = alpha2theta(alpha)\n",
    "            return ordinal_logistic_nll(y_true=y_true, y_preds=y_preds, theta=theta)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _optimise_alpha(self, y_true, y_preds):\n",
    "        \"\"\"\n",
    "        Takes loss parametrised by alpha and optimises it.\n",
    "        Can optionally take in gradient.\n",
    "        \"\"\"\n",
    "        loss = self._alpha_loss_factory(y_true, y_preds)\n",
    "        alpha = theta2alpha(self.theta)\n",
    "        bounds = [(None,3.58)]*len(alpha)\n",
    "        self._alpha_optimisation_report = minimize(loss, alpha, bounds=bounds)\n",
    "        alpha = self._alpha_optimisation_report.x\n",
    "        self.theta = alpha2theta(alpha)\n",
    "\n",
    "    def _initialise_objective(self, y):\n",
    "        \"\"\"\n",
    "        initialises the objective by creating the loss and setting the class\n",
    "        attributes\n",
    "        \"\"\"\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.objective = self._lgb_loss_factory()\n",
    "        self._objective = self.objective\n",
    "\n",
    "    def _output_to_probability(self, output):\n",
    "        return probas_from_y_pred(output, self.theta)\n",
    "\n",
    "    def _hot_start(self, X, y, hot_start_iterations=5, **kwargs):\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        fit_n_estimators = self.n_estimators\n",
    "        self.n_estimators = hot_start_iterations\n",
    "\n",
    "        # Fits the model for the default initialisation of alphas\n",
    "        self._fit(X, y, **kwargs)\n",
    "\n",
    "        # Updates the alpha to those that minimise the loss\n",
    "        self._optimise_alpha(y, self.predict_proba(X, raw_score=True))\n",
    "        self._Booster = None\n",
    "        self.n_estimators = fit_n_estimators\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        hot_start_iterations=5,\n",
    "        sample_weight=None,\n",
    "        init_score=None,\n",
    "        eval_set=None,\n",
    "        eval_names=None,\n",
    "        eval_sample_weight=None,\n",
    "        eval_init_score=None,\n",
    "        eval_metric=None,\n",
    "        early_stopping_rounds=None,\n",
    "        verbose=\"warn\",\n",
    "        feature_name=\"auto\",\n",
    "        categorical_feature=\"auto\",\n",
    "        callbacks=None,\n",
    "        init_model=None,\n",
    "    ) -> \"LGBMOrdinal\":\n",
    "        \"\"\"Docstring is inherited from the LGBMModel.\"\"\"\n",
    "        self._initialise_objective(y)\n",
    "        self._hot_start(X, y, hot_start_iterations=hot_start_iterations)\n",
    "        self._fit(\n",
    "            X,\n",
    "            y,\n",
    "            sample_weight=sample_weight,\n",
    "            init_score=init_score,\n",
    "            eval_set=eval_set,\n",
    "            eval_names=eval_names,\n",
    "            eval_sample_weight=eval_sample_weight,\n",
    "            eval_init_score=eval_init_score,\n",
    "            eval_metric=eval_metric,\n",
    "            feature_name=feature_name,\n",
    "            categorical_feature=categorical_feature,\n",
    "            callbacks=callbacks,\n",
    "            init_model=init_model,\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def _fit(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        sample_weight=None,\n",
    "        init_score=None,\n",
    "        eval_set=None,\n",
    "        eval_names=None,\n",
    "        eval_sample_weight=None,\n",
    "        eval_init_score=None,\n",
    "        eval_metric=None,\n",
    "        early_stopping_rounds=None,\n",
    "        verbose=\"warn\",\n",
    "        feature_name=\"auto\",\n",
    "        categorical_feature=\"auto\",\n",
    "        callbacks=None,\n",
    "        init_model=None,\n",
    "    ) -> \"LGBMOrdinal\":\n",
    "        \"\"\"Docstring is inherited from the LGBMModel.\"\"\"\n",
    "        self = super().fit(\n",
    "            X,\n",
    "            y,\n",
    "            sample_weight=sample_weight,\n",
    "            init_score=init_score,\n",
    "            eval_set=eval_set,\n",
    "            eval_names=eval_names,\n",
    "            eval_sample_weight=eval_sample_weight,\n",
    "            eval_init_score=eval_init_score,\n",
    "            eval_metric=eval_metric,\n",
    "            feature_name=feature_name,\n",
    "            categorical_feature=categorical_feature,\n",
    "            callbacks=callbacks,\n",
    "            init_model=init_model,\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        X,\n",
    "        start_iteration=0,\n",
    "        num_iteration=None,\n",
    "        pred_leaf=False,\n",
    "        pred_contrib=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        preds = self.predict_proba(\n",
    "            X,\n",
    "            raw_score=False,\n",
    "            start_iteration=start_iteration,\n",
    "            num_iteration=num_iteration,\n",
    "            pred_leaf=pred_leaf,\n",
    "            pred_contrib=pred_contrib,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return np.argmax(preds, axis=1)\n",
    "\n",
    "    def predict_proba(\n",
    "        self,\n",
    "        X,\n",
    "        raw_score=False,\n",
    "        start_iteration=0,\n",
    "        num_iteration=None,\n",
    "        pred_leaf=False,\n",
    "        pred_contrib=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        preds = super().predict(\n",
    "            X,\n",
    "            raw_score=raw_score,\n",
    "            start_iteration=start_iteration,\n",
    "            num_iteration=num_iteration,\n",
    "            pred_leaf=pred_leaf,\n",
    "            pred_contrib=pred_contrib,\n",
    "            **kwargs,\n",
    "        )\n",
    "        if not raw_score:\n",
    "            return self._output_to_probability(preds)\n",
    "        return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a897eff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:39:03.267012Z",
     "iopub.status.busy": "2024-10-07T01:39:03.265546Z",
     "iopub.status.idle": "2024-10-07T01:39:03.591725Z",
     "shell.execute_reply": "2024-10-07T01:39:03.590603Z"
    },
    "papermill": {
     "duration": 0.339481,
     "end_time": "2024-10-07T01:39:03.595055",
     "exception": false,
     "start_time": "2024-10-07T01:39:03.255574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5f2c42f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:39:03.620217Z",
     "iopub.status.busy": "2024-10-07T01:39:03.619076Z",
     "iopub.status.idle": "2024-10-07T01:39:03.649338Z",
     "shell.execute_reply": "2024-10-07T01:39:03.647979Z"
    },
    "papermill": {
     "duration": 0.04554,
     "end_time": "2024-10-07T01:39:03.651865",
     "exception": false,
     "start_time": "2024-10-07T01:39:03.606325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(true: np.ndarray, predicted: np.ndarray) -> np.float64:\n",
    "    true_binned = true\n",
    "    predicted_binned = predicted\n",
    "    \n",
    "    # Ensure that true_binned and predicted_binned arrays have the same length\n",
    "    assert len(true_binned) == len(predicted_binned), \"The true and predicted arrays must have the same length\"\n",
    "    \n",
    "    # Define the number of distinct bins\n",
    "    N = len(np.unique(true_binned))\n",
    "    \n",
    "    # Create the histogram matrix O\n",
    "    O = np.zeros((N, N), dtype=np.float64)\n",
    "    for t, p in zip(true_binned, predicted_binned):\n",
    "        O[t, p] += 1\n",
    "    \n",
    "    # Create the weight matrix W\n",
    "    W = np.zeros((N, N), dtype=np.float64)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            W[i, j] = ((i - j) ** 2) / ((N - 1) ** 2)\n",
    "    \n",
    "    # Create histograms of the true and predicted values\n",
    "    true_hist = np.bincount(true_binned, minlength=N)\n",
    "    pred_hist = np.bincount(predicted_binned, minlength=N)\n",
    "    \n",
    "    # Expected matrix E as the outer product of histograms, normalized\n",
    "    E = np.outer(true_hist, pred_hist) / len(true_binned)\n",
    "    \n",
    "    # Normalize O and E to have the same sum\n",
    "    O_sum = np.sum(O)\n",
    "    E_sum = np.sum(E)\n",
    "    \n",
    "    O_normalized = O / O_sum\n",
    "    E_normalized = E / E_sum\n",
    "    \n",
    "    # Calculate the numerator and denominator of the quadratic weighted kappa\n",
    "    num = np.sum(W * O_normalized)\n",
    "    denom = np.sum(W * E_normalized)\n",
    "    \n",
    "    # Compute the quadratic weighted kappa\n",
    "    kappa = 1 - (num / denom)\n",
    "    \n",
    "    return kappa\n",
    "\n",
    "def pre_process_train(main):\n",
    "    one_hot_columns = [\"Basic_Demos-Enroll_Season\",\n",
    "                                    \"CGAS-Season\",\n",
    "                                    \"Physical-Season\",\n",
    "                                    \"Fitness_Endurance-Season\",\n",
    "                                    \"FGC-Season\",\n",
    "                                    \"BIA-Season\",\n",
    "                                    \"PAQ_A-Season\",\n",
    "                                    \"PAQ_C-Season\",\n",
    "                                    \"PCIAT-Season\",\n",
    "                                    \"SDS-Season\",\n",
    "                                    \"PreInt_EduHx-Season\"]\n",
    "    one_hot_encoded = pd.get_dummies(main.to_pandas(), columns=one_hot_columns\n",
    "                                    ,dummy_na=True)\n",
    "    \n",
    "    return one_hot_encoded\n",
    "\n",
    "def pre_process_test(main):\n",
    "    one_hot_columns = [\"Basic_Demos-Enroll_Season\",\n",
    "                                    \"CGAS-Season\",\n",
    "                                    \"Physical-Season\",\n",
    "                                    \"Fitness_Endurance-Season\",\n",
    "                                    \"FGC-Season\",\n",
    "                                    \"BIA-Season\",\n",
    "                                    \"PAQ_A-Season\",\n",
    "                                    \"PAQ_C-Season\",\n",
    "                                    \"SDS-Season\",\n",
    "                                    \"PreInt_EduHx-Season\"]\n",
    "    one_hot_encoded = pd.get_dummies(main.to_pandas(), columns=one_hot_columns\n",
    "                                    ,dummy_na=True)\n",
    "    \n",
    "    return one_hot_encoded\n",
    "\n",
    "def load_actigraphy_train() -> pl.DataFrame:\n",
    "    # Load the data\n",
    "    df = pl.scan_parquet(\"../input/child-mind-institute-problematic-internet-use/series_train.parquet\", hive_partitioning=True)\n",
    "    non_wear_pct = df.group_by(\"id\").agg(pl.col(\"non-wear_flag\").mean())\n",
    "    good_data = df.filter(pl.col(\"non-wear_flag\") == 0)\n",
    "    averages = good_data.group_by(\"id\").agg(\n",
    "        non_wear_pct=pl.col(\"non-wear_flag\").mean(),\n",
    "        enmo=pl.col(\"enmo\").mean(),\n",
    "        peak_enmo=pl.col(\"enmo\").max(),\n",
    "        anglez=pl.col(\"anglez\").mean(),\n",
    "        light=pl.col(\"light\").mean(),\n",
    "        peak_light=pl.col(\"light\").max(),\n",
    "    )\n",
    "    averages = averages.join(non_wear_pct, on=\"id\", how=\"left\")\n",
    "\n",
    "    return averages.collect(streaming=True)\n",
    "\n",
    "def load_main_train() -> pl.DataFrame:\n",
    "    # Load the data\n",
    "    df = pl.read_csv(\"../input/child-mind-institute-problematic-internet-use/train.csv\")\n",
    "    return df\n",
    "\n",
    "def load_actigraphy_test() -> pl.DataFrame:\n",
    "    # Load the data\n",
    "    df = pl.scan_parquet(\"../input/child-mind-institute-problematic-internet-use/series_test.parquet\", hive_partitioning=True)\n",
    "    non_wear_pct = df.group_by(\"id\").agg(pl.col(\"non-wear_flag\").mean())\n",
    "    good_data = df.filter(pl.col(\"non-wear_flag\") == 0)\n",
    "    averages = good_data.group_by(\"id\").agg(\n",
    "        non_wear_pct=pl.col(\"non-wear_flag\").mean(),\n",
    "        enmo=pl.col(\"enmo\").mean(),\n",
    "        peak_enmo=pl.col(\"enmo\").max(),\n",
    "        anglez=pl.col(\"anglez\").mean(),\n",
    "        light=pl.col(\"light\").mean(),\n",
    "        peak_light=pl.col(\"light\").max(),\n",
    "    )\n",
    "    averages = averages.join(non_wear_pct, on=\"id\", how=\"left\")\n",
    "\n",
    "    return averages.collect(streaming=True)\n",
    "\n",
    "def load_main_test() -> pl.DataFrame:\n",
    "    # Load the data\n",
    "    df = pl.read_csv(\"../input/child-mind-institute-problematic-internet-use/test.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_train() -> pl.DataFrame:\n",
    "    avgs = load_actigraphy_train()\n",
    "    main = pl.DataFrame(pre_process_train(load_main_train()))\n",
    "    merged = main.join(avgs, on=\"id\", how=\"left\")\n",
    "    return merged.to_pandas()\n",
    "\n",
    "def load_test() -> pl.DataFrame:\n",
    "    avgs = load_actigraphy_test()\n",
    "    main = pl.DataFrame(pre_process_test(load_main_test()))\n",
    "    merged = main.join(avgs, on=\"id\", how='left')\n",
    "    return merged.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3bf6a97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:39:03.670860Z",
     "iopub.status.busy": "2024-10-07T01:39:03.670324Z",
     "iopub.status.idle": "2024-10-07T01:41:06.391905Z",
     "shell.execute_reply": "2024-10-07T01:41:06.390197Z"
    },
    "papermill": {
     "duration": 122.734717,
     "end_time": "2024-10-07T01:41:06.395199",
     "exception": false,
     "start_time": "2024-10-07T01:39:03.660482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = load_train()\n",
    "data_test = load_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1110058",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:41:06.412985Z",
     "iopub.status.busy": "2024-10-07T01:41:06.412550Z",
     "iopub.status.idle": "2024-10-07T01:41:06.431311Z",
     "shell.execute_reply": "2024-10-07T01:41:06.430016Z"
    },
    "papermill": {
     "duration": 0.031317,
     "end_time": "2024-10-07T01:41:06.434641",
     "exception": false,
     "start_time": "2024-10-07T01:41:06.403324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "pciat_cols = ['PCIAT-PCIAT_01',\n",
    " 'PCIAT-PCIAT_02',\n",
    " 'PCIAT-PCIAT_03',\n",
    " 'PCIAT-PCIAT_04',\n",
    " 'PCIAT-PCIAT_05',\n",
    " 'PCIAT-PCIAT_06',\n",
    " 'PCIAT-PCIAT_07',\n",
    " 'PCIAT-PCIAT_08',\n",
    " 'PCIAT-PCIAT_09',\n",
    " 'PCIAT-PCIAT_10',\n",
    " 'PCIAT-PCIAT_11',\n",
    " 'PCIAT-PCIAT_12',\n",
    " 'PCIAT-PCIAT_13',\n",
    " 'PCIAT-PCIAT_14',\n",
    " 'PCIAT-PCIAT_15',\n",
    " 'PCIAT-PCIAT_16',\n",
    " 'PCIAT-PCIAT_17',\n",
    " 'PCIAT-PCIAT_18',\n",
    " 'PCIAT-PCIAT_19',\n",
    " 'PCIAT-PCIAT_20',]\n",
    "target = data[pciat_cols].dropna()\n",
    "train = data.dropna(subset=pciat_cols)[data_test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4ce3bd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:41:06.451856Z",
     "iopub.status.busy": "2024-10-07T01:41:06.451432Z",
     "iopub.status.idle": "2024-10-07T01:41:06.457474Z",
     "shell.execute_reply": "2024-10-07T01:41:06.456069Z"
    },
    "papermill": {
     "duration": 0.017854,
     "end_time": "2024-10-07T01:41:06.460280",
     "exception": false,
     "start_time": "2024-10-07T01:41:06.442426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecf7a21c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:41:06.478708Z",
     "iopub.status.busy": "2024-10-07T01:41:06.478262Z",
     "iopub.status.idle": "2024-10-07T01:41:08.022327Z",
     "shell.execute_reply": "2024-10-07T01:41:08.020593Z"
    },
    "papermill": {
     "duration": 1.557261,
     "end_time": "2024-10-07T01:41:08.025286",
     "exception": false,
     "start_time": "2024-10-07T01:41:06.468025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5108713203436928\n",
      "0.8254459149879478\n",
      "0.5675071751547364\n",
      "0.7629324218962888\n",
      "0.5284183095433783\n",
      "0.5255786885143026\n",
      "0.8080040946563061\n",
      "0.7902768147884178\n",
      "0.4651564649823169\n",
      "0.533428880842617\n",
      "0.5630137726678217\n",
      "0.8534248241464375\n",
      "0.7694546816215129\n",
      "0.7370210067189135\n",
      "0.7863558228465308\n",
      "0.7682522028516778\n",
      "0.5892491986983842\n",
      "0.452257470298842\n",
      "0.774021382450578\n",
      "0.38709344842334603\n"
     ]
    }
   ],
   "source": [
    "pciat_out = pl.DataFrame()\n",
    "pciat_models = {}\n",
    "for col in pciat_cols:\n",
    "    y = target[col].values.astype(int)\n",
    "    X = train.drop('id', axis=1).values\n",
    "    model = joblib.load(f'../input/pciat-models/{col}.joblib')\n",
    "    pciat_models[col] = model\n",
    "    y_hat = model.predict(X)\n",
    "    print(quadratic_weighted_kappa(y, y_hat))\n",
    "    pciat_out = pciat_out.with_columns(pl.Series(name=col, values=y_hat))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b34350e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:41:08.044798Z",
     "iopub.status.busy": "2024-10-07T01:41:08.044353Z",
     "iopub.status.idle": "2024-10-07T01:41:08.058438Z",
     "shell.execute_reply": "2024-10-07T01:41:08.057274Z"
    },
    "papermill": {
     "duration": 0.027242,
     "end_time": "2024-10-07T01:41:08.061475",
     "exception": false,
     "start_time": "2024-10-07T01:41:08.034233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "pciat_out = pciat_out.with_columns(pl.Series(name='PCIAT-TOTAL', values=pciat_out.sum_horizontal().to_numpy()))\n",
    "pciat_out = pciat_out.with_columns(pl.Series(name='id', values=train['id'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66669c4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:41:08.080911Z",
     "iopub.status.busy": "2024-10-07T01:41:08.080464Z",
     "iopub.status.idle": "2024-10-07T01:41:08.086675Z",
     "shell.execute_reply": "2024-10-07T01:41:08.085454Z"
    },
    "papermill": {
     "duration": 0.019312,
     "end_time": "2024-10-07T01:41:08.089295",
     "exception": false,
     "start_time": "2024-10-07T01:41:08.069983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pciat_to_sii(x):\n",
    "    if x < 31:\n",
    "        return 0\n",
    "    elif x < 49:\n",
    "        return 1\n",
    "    elif x < 79:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "556d9b21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:41:08.108964Z",
     "iopub.status.busy": "2024-10-07T01:41:08.108500Z",
     "iopub.status.idle": "2024-10-07T01:41:08.127293Z",
     "shell.execute_reply": "2024-10-07T01:41:08.125794Z"
    },
    "papermill": {
     "duration": 0.031776,
     "end_time": "2024-10-07T01:41:08.129901",
     "exception": false,
     "start_time": "2024-10-07T01:41:08.098125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2_671, 22)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>PCIAT-PCIAT_01</th><th>PCIAT-PCIAT_02</th><th>PCIAT-PCIAT_03</th><th>PCIAT-PCIAT_04</th><th>PCIAT-PCIAT_05</th><th>PCIAT-PCIAT_06</th><th>PCIAT-PCIAT_07</th><th>PCIAT-PCIAT_08</th><th>PCIAT-PCIAT_09</th><th>PCIAT-PCIAT_10</th><th>PCIAT-PCIAT_11</th><th>PCIAT-PCIAT_12</th><th>PCIAT-PCIAT_13</th><th>PCIAT-PCIAT_14</th><th>PCIAT-PCIAT_15</th><th>PCIAT-PCIAT_16</th><th>PCIAT-PCIAT_17</th><th>PCIAT-PCIAT_18</th><th>PCIAT-PCIAT_19</th><th>PCIAT-PCIAT_20</th><th>PCIAT-TOTAL</th><th>id</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>2</td><td>4</td><td>4</td><td>0</td><td>0</td><td>0</td><td>0</td><td>4</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>1</td><td>2</td><td>2</td><td>4</td><td>2</td><td>0</td><td>0</td><td>27</td><td>&quot;00008ff9&quot;</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&quot;000fd460&quot;</td></tr><tr><td>2</td><td>2</td><td>2</td><td>0</td><td>2</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>2</td><td>0</td><td>1</td><td>15</td><td>&quot;00105258&quot;</td></tr><tr><td>0</td><td>2</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>10</td><td>&quot;00115b9f&quot;</td></tr><tr><td>2</td><td>4</td><td>2</td><td>0</td><td>2</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>2</td><td>2</td><td>0</td><td>0</td><td>2</td><td>0</td><td>1</td><td>0</td><td>21</td><td>&quot;001f3379&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2</td><td>4</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>1</td><td>0</td><td>14</td><td>&quot;ff6c2bb8&quot;</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>3</td><td>&quot;ff759544&quot;</td></tr><tr><td>2</td><td>4</td><td>4</td><td>1</td><td>2</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>2</td><td>0</td><td>1</td><td>0</td><td>2</td><td>0</td><td>2</td><td>2</td><td>0</td><td>0</td><td>26</td><td>&quot;ff8a2de4&quot;</td></tr><tr><td>4</td><td>5</td><td>4</td><td>0</td><td>5</td><td>1</td><td>0</td><td>1</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>24</td><td>&quot;ffcd4dbd&quot;</td></tr><tr><td>4</td><td>0</td><td>2</td><td>1</td><td>5</td><td>0</td><td>0</td><td>0</td><td>1</td><td>2</td><td>4</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>2</td><td>0</td><td>1</td><td>24</td><td>&quot;ffed1dd5&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2_671, 22)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ PCIAT-PCI ┆ PCIAT-PCI ┆ PCIAT-PCI ┆ PCIAT-PCI ┆ … ┆ PCIAT-PCI ┆ PCIAT-PCI ┆ PCIAT-TOT ┆ id       │\n",
       "│ AT_01     ┆ AT_02     ┆ AT_03     ┆ AT_04     ┆   ┆ AT_19     ┆ AT_20     ┆ AL        ┆ ---      │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ str      │\n",
       "│ i64       ┆ i64       ┆ i64       ┆ i64       ┆   ┆ i64       ┆ i64       ┆ i64       ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 2         ┆ 4         ┆ 4         ┆ 0         ┆ … ┆ 0         ┆ 0         ┆ 27        ┆ 00008ff9 │\n",
       "│ 0         ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0         ┆ 0         ┆ 000fd460 │\n",
       "│ 2         ┆ 2         ┆ 2         ┆ 0         ┆ … ┆ 0         ┆ 1         ┆ 15        ┆ 00105258 │\n",
       "│ 0         ┆ 2         ┆ 2         ┆ 0         ┆ … ┆ 1         ┆ 0         ┆ 10        ┆ 00115b9f │\n",
       "│ 2         ┆ 4         ┆ 2         ┆ 0         ┆ … ┆ 1         ┆ 0         ┆ 21        ┆ 001f3379 │\n",
       "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       "│ 2         ┆ 4         ┆ 2         ┆ 0         ┆ … ┆ 1         ┆ 0         ┆ 14        ┆ ff6c2bb8 │\n",
       "│ 0         ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0         ┆ 3         ┆ ff759544 │\n",
       "│ 2         ┆ 4         ┆ 4         ┆ 1         ┆ … ┆ 0         ┆ 0         ┆ 26        ┆ ff8a2de4 │\n",
       "│ 4         ┆ 5         ┆ 4         ┆ 0         ┆ … ┆ 0         ┆ 0         ┆ 24        ┆ ffcd4dbd │\n",
       "│ 4         ┆ 0         ┆ 2         ┆ 1         ┆ … ┆ 0         ┆ 1         ┆ 24        ┆ ffed1dd5 │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pciat_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3299261f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:41:08.149764Z",
     "iopub.status.busy": "2024-10-07T01:41:08.149326Z",
     "iopub.status.idle": "2024-10-07T01:41:08.164149Z",
     "shell.execute_reply": "2024-10-07T01:41:08.162739Z"
    },
    "papermill": {
     "duration": 0.028103,
     "end_time": "2024-10-07T01:41:08.166915",
     "exception": false,
     "start_time": "2024-10-07T01:41:08.138812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sii_hat = pciat_out['PCIAT-TOTAL'].map_elements(pciat_to_sii, return_dtype=pl.Int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "514e29d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:41:08.186534Z",
     "iopub.status.busy": "2024-10-07T01:41:08.186091Z",
     "iopub.status.idle": "2024-10-07T01:41:08.199970Z",
     "shell.execute_reply": "2024-10-07T01:41:08.198741Z"
    },
    "papermill": {
     "duration": 0.026883,
     "end_time": "2024-10-07T01:41:08.202654",
     "exception": false,
     "start_time": "2024-10-07T01:41:08.175771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>PCIAT-TOTAL</th><th>count</th></tr><tr><td>i64</td><td>u32</td></tr></thead><tbody><tr><td>2</td><td>158</td></tr><tr><td>0</td><td>2123</td></tr><tr><td>3</td><td>10</td></tr><tr><td>1</td><td>380</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 2)\n",
       "┌─────────────┬───────┐\n",
       "│ PCIAT-TOTAL ┆ count │\n",
       "│ ---         ┆ ---   │\n",
       "│ i64         ┆ u32   │\n",
       "╞═════════════╪═══════╡\n",
       "│ 2           ┆ 158   │\n",
       "│ 0           ┆ 2123  │\n",
       "│ 3           ┆ 10    │\n",
       "│ 1           ┆ 380   │\n",
       "└─────────────┴───────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sii_hat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91254435",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:41:08.222720Z",
     "iopub.status.busy": "2024-10-07T01:41:08.222279Z",
     "iopub.status.idle": "2024-10-07T01:41:08.234178Z",
     "shell.execute_reply": "2024-10-07T01:41:08.233067Z"
    },
    "papermill": {
     "duration": 0.025205,
     "end_time": "2024-10-07T01:41:08.236860",
     "exception": false,
     "start_time": "2024-10-07T01:41:08.211655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sii = target[pciat_cols].sum(axis=1).apply(pciat_to_sii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1efbeaf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:41:08.257549Z",
     "iopub.status.busy": "2024-10-07T01:41:08.257109Z",
     "iopub.status.idle": "2024-10-07T01:41:08.268373Z",
     "shell.execute_reply": "2024-10-07T01:41:08.267009Z"
    },
    "papermill": {
     "duration": 0.024609,
     "end_time": "2024-10-07T01:41:08.270775",
     "exception": false,
     "start_time": "2024-10-07T01:41:08.246166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set QWK: 0.6112416229260772\n"
     ]
    }
   ],
   "source": [
    "print(f'Train set QWK: {quadratic_weighted_kappa(sii, sii_hat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "925c73af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:41:08.292080Z",
     "iopub.status.busy": "2024-10-07T01:41:08.290169Z",
     "iopub.status.idle": "2024-10-07T01:41:08.352220Z",
     "shell.execute_reply": "2024-10-07T01:41:08.350868Z"
    },
    "papermill": {
     "duration": 0.075771,
     "end_time": "2024-10-07T01:41:08.355558",
     "exception": false,
     "start_time": "2024-10-07T01:41:08.279787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pciat_test = pl.DataFrame()\n",
    "for col in pciat_cols:\n",
    "    X = data_test.drop('id', axis=1).values\n",
    "    y_hat = pciat_models[col].predict(X)\n",
    "    pciat_test = pciat_test.with_columns(pl.Series(name=col, values=y_hat))\n",
    "\n",
    "pciat_test = pciat_test.with_columns(pl.Series(name='PCIAT-TOTAL', values=pciat_test.sum_horizontal().to_numpy()))\n",
    "pciat_test = pciat_test.with_columns(pl.Series(name='id', values=data_test['id'].values))\n",
    "pciat_test = pciat_test.with_columns(pl.Series(name='sii', values=pciat_test['PCIAT-TOTAL'].map_elements(pciat_to_sii, return_dtype=pl.Float64)))\n",
    "output_file = 'submission.csv'\n",
    "output = pciat_test.select(pl.col('id'), pl.col('sii')).to_pandas()\n",
    "output['sii'] = output['sii'].astype(int)\n",
    "output.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    },
    {
     "datasetId": 5828182,
     "sourceId": 9563456,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5828215,
     "sourceId": 9563517,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 150.122456,
   "end_time": "2024-10-07T01:41:09.188974",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-07T01:38:39.066518",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
